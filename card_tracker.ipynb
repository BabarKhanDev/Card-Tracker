{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from helpers import *\n",
    "import database as Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "card_dirs = os.listdir(\"cards/\")\n",
    "\n",
    "create_homography_for_all_cards(card_dirs)\n",
    "calculate_features_for_all_cards(card_dirs)\n",
    "\n",
    "# THIS WILL RESET THE DATABASe\n",
    "database = Database.Database()\n",
    "# THE ABOVE WILL RESET THE DATABASE\n",
    "\n",
    "#database = load_database()\n",
    "database.add_new_cards()\n",
    "save_database(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Database Object and save\n",
    "sorted_cards = database.sorted_cards\n",
    "card_groups = database.card_groups\n",
    "database = Database.Database()\n",
    "database.sorted_cards = sorted_cards\n",
    "database.card_groups = card_groups\n",
    "save_database(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_to_cards(database):\n",
    "    group_to_cards = {}\n",
    "    for k,v in database.card_groups.items():\n",
    "        if v in group_to_cards:\n",
    "            group_to_cards[v].append(k)\n",
    "        else:\n",
    "            group_to_cards[v] = [k]\n",
    "\n",
    "    return group_to_cards\n",
    "\n",
    "def visualise_group(database, group, group_to_cards):\n",
    "    group_i = group_to_cards[group]\n",
    "    for i, path in enumerate(group_i):\n",
    "        plt.subplot(1, len(group_i), i+1)\n",
    "        image = Image.open(f\"homography_cards/{path}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all copies of multiple cards\n",
    "group_to_cards = get_group_to_cards(database)\n",
    "multi_card_groups = [k for k,v in group_to_cards.items() if len(v) > 1]\n",
    "print(f\"There are {len(multi_card_groups)} cards that you have duplicates of:\")\n",
    "for group in multi_card_groups:\n",
    "    print(group_to_cards[group])\n",
    "    visualise_group(database, group, group_to_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 78/151 [25:49<24:10, 19.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legendary-Collection-Expansion Scoop-Up-Card-104.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Features for all cards scraped from online\n",
    "from helpers import extract_features, create_model_and_preprocess\n",
    "\n",
    "model, preprocess = create_model_and_preprocess()\n",
    "\n",
    "try:\n",
    "    all_sets = os.listdir(\"card_db/sets\")\n",
    "    for set in tqdm(all_sets):\n",
    "\n",
    "        # make a directory for the set if it doesn't exist\n",
    "        if not os.path.exists(os.path.join(os.getcwd(), f\"card_db\\\\features\", set)):\n",
    "            os.makedirs(f\"card_db/features/{set}\")\n",
    "        \n",
    "        # make a directory for the set if it doesn't exist\n",
    "        if not os.path.exists(os.path.join(os.getcwd(), f\"card_db\\\\homography_cards\", set)):\n",
    "            os.makedirs(f\"card_db/homography_cards/{set}\")\n",
    "\n",
    "        # Loop through all cards in a set\n",
    "        all_cards = os.listdir(f\"card_db/sets/{set}\")\n",
    "        for card in all_cards:\n",
    "            \n",
    "            # If we have already computed the features then skip this card\n",
    "            if os.path.exists(os.path.join(os.getcwd(), f\"card_db\\\\features\\\\{set}\", f\"{card[:-4]}.pkl\")):\n",
    "                continue\n",
    "            \n",
    "            # Add a border and rotate the image a bit, otherwise we get infinite gradient lines\n",
    "            img = cv.imread(f\"card_db/sets/{set}/{card}\")\n",
    "            border_size = 20\n",
    "            border_color = [0,0,0]\n",
    "            img = cv.copyMakeBorder(img, border_size, border_size, border_size, border_size, cv.BORDER_CONSTANT, value=border_color)\n",
    "\n",
    "            # rotation matrix\n",
    "            height, width = img.shape[:2]\n",
    "            center = (width // 2, height // 2)\n",
    "            angle = 5\n",
    "            M = cv.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "            # compute the new image size\n",
    "            cos = np.abs(M[0, 0])\n",
    "            sin = np.abs(M[0, 1])\n",
    "            new_width = int((height * sin) + (width * cos))\n",
    "            new_height = int((height * cos) + (width * sin))\n",
    "\n",
    "            # adjust the rotation matrix to take into account the translation\n",
    "            M[0, 2] += (new_width / 2) - center[0]\n",
    "            M[1, 2] += (new_height / 2) - center[1]\n",
    "            \n",
    "            # perform the rotation\n",
    "            img = cv.warpAffine(img, M, (new_width, new_height))\n",
    "\n",
    "            # Find the corners of the card, compute the homography, and resave it\n",
    "            _, edges = detect_edges(img)\n",
    "            pts_src = corners_from_edges(edges)\n",
    "            card_homography = compute_homography(pts_src, img)\n",
    "            card_homography = cv.resize(card_homography, (300, 400), card_homography, interpolation=cv.INTER_AREA)\n",
    "            cv.imwrite(f\"card_db/homography_cards/{set}/{card}\", card_homography)\n",
    "            \n",
    "            # Calculate the features of the card and dump them\n",
    "            features = extract_features(model, preprocess, f\"card_db/homography_cards/{set}/{card}\")\n",
    "            with open(f\"card_db/features/{set}/{card[:-4]}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(features, file)\n",
    "except:\n",
    "    print(set, card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cards where there were no images on the website\n",
    "model, preprocess = create_model_and_preprocess()\n",
    "\n",
    "card_dir = \"card_db/pokellector_empty.jpg\"\n",
    "img = cv.imread(card_dir)\n",
    "\n",
    "all_sets = os.listdir(\"card_db/sets\")\n",
    "for set in tqdm(all_sets):\n",
    "\n",
    "    # Loop through all cards in a set\n",
    "    all_cards = os.listdir(f\"card_db/sets/{set}\")\n",
    "    for card in all_cards:\n",
    "        try:\n",
    "            img2 = cv.imread(f\"card_db/sets/{set}/{card}\")\n",
    "            if np.sum(img2-img) == 0:\n",
    "                os.remove(f\"card_db/sets/{set}/{card}\")\n",
    "                with open(\"card_db/blacklist.txt\", \"a\") as file:\n",
    "                    file.write(f\"{set}/{card}\")\n",
    "        except:\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
